## SAM

------

<!-- https://blog.csdn.net/leiduifan6944/article/details/130080159 -->
<!-- SAM,SAM survy,医疗影像的SAM -->

<!-- https://zhuanlan.zhihu.com/p/619962145 -->

<!-- 医疗SAM -->
<!-- https://zhuanlan.zhihu.com/p/625451983 -->

<!-- https://zhuanlan.zhihu.com/p/625171421 -->
<!-- https://zhuanlan.zhihu.com/p/623842237 -->
<!-- https://zhuanlan.zhihu.com/p/626436893 -->
<!-- https://blog.csdn.net/moxibingdao/article/details/130538108 -->

<!-- https://mp.weixin.qq.com/s/m5R5RIINTMaZlCVrWJKCGg
https://mp.weixin.qq.com/s/lITbYzGlqNozAQh4XKr-4Q

徐静 (Jill Tsui):
https://arxiv.org/pdf/2304.12306.pdf

徐静 (Jill Tsui):
https://arxiv.org/pdf/2304.14660.pdf

徐静 (Jill Tsui):
https://arxiv.org/abs/2304.12620

徐静 (Jill Tsui):
https://arxiv.org/pdf/2304.09324.pdf


SAM综述
https://mp.weixin.qq.com/s/39imonlyIdSHYW9VnQhOjw
https://mp.weixin.qq.com/s/l7tf8mgUnm3-RtcS_C1b-w -->

### 1. Segment Anything

!> github: https://github.com/facebookresearch/segment-anything

!> homepage: https://segment-anything.com/

#### 摘要

我们介绍Segment Anything（SA）项目：一种新的图像分割任务、模型和数据集。在数据采集循环中使用我们的高效模型，我们建立了迄今为止最大的分割数据集，在1100万许可和尊重隐私的图像上有超过10亿个掩码。该模型被设计和训练为可提示的，因此它可以将zero-shot transfer零样本迁移到新的图像分布和任务。我们评估了它在许多任务上的能力，并发现它的零样本性能令人印象深刻，通常与之前的完全监督的结果相当，甚至更优。我们正在发布Segment Anything Model（SAM）和相应的数据集（SA-1B），其中包含10亿个掩码和1100万个图像，以促进计算机视觉基础模型的研究。

<div align=center>
    <img src="zh-cn/img/ch3/p1.png" /> 
</div><p align=center>图1. 我们的目标是建立一个基础模型分割通过引入三个相互关联的组件：一个敏捷的分割任务，分割模型（SAM）权力数据标注和使零样本迁移通过一系列任务提示工程，和数据引擎收集SA-1B，我们的数据集超过10亿的掩码。</p>

#### 1.引言(Introduction)

在网络规模数据集上预训练的大语言模型正在NLP领域引起革命，具有强大的zero-shot零样本和few-shot少量样本泛化能力。这些“基础模型”可以泛化到训练期间看不到的任务和数据分布。这种能力通常通过prompt engineering提示工程来实现，其中手工制作的文本用于提示语言模型为任务生成有效的文本响应。当规模扩大并使用来自网络的丰富文本语料库进行训练时，这些模型的零样本和f少量性能令人印象深刻，通常与（甚至在某些情况下与）微调模型相当。经验趋势表明，随着模型规模、数据集大小和总训练计算量的增加，这种行为会有所改善。

计算机视觉领域也探索了基础模型，尽管程度较小。也许最突出的例子是从网络中对齐配对的文本和图像。例如，CLIP和ALIGN使用对比学习来训练文本和图像编码器，以对齐两种模态。一旦训练完成，工程化的文本提示就可以将零样本泛化到新的视觉概念和数据分布。这些编码器也可以与其他模块有效组合，以实现下游任务，例如图像生成（例如，DALL·E）。虽然在视觉和语言编码器方面取得了很大进展，但计算机视觉领域包含了远远超出这个范围的问题，对于其中的许多问题，都不存在充足的训练数据。

在这项工作中，我们的目标是为图像分割构建一个foundation model基础模型。也就是说，我们希望开发一个可提示的模型，并使用一种任务在广泛的数据集上对其进行预训练，以实现强大的泛化。有了这个模型，我们的目标是使用提示工程来解决一系列下游分割问题，以解决新的数据分布问题。

这个计划的成功取决于三个组件：task任务、model模型和data数据。为了开发它们，我们解决了关于图像分割的以下问题：

1. 是什么task任务可以实现零样本泛化？
2. 对应的model模型架构是什么？
3. 哪些data数据集可以支持这项任务和模型？

这些问题是相互联系的，需要综合解决。我们首先定义了一个promptable segmentation task可提示的分割任务，该任务足够通用，可以提供强大的预训练目标，并且可以启用广泛的下游应用程序。这项任务需要一个支持灵活提示（supports flexible prompting）的模型，并且可以在提示时以实时方式输出分割掩码，以便进行交互式使用。为了训练我们的模型，我们需要一个多样化的、大规模的data数据源。不幸的是，还没有用于分割的网络规模数据源；为了解决这个问题，我们建立了一个“data engine数据引擎”，即我们在使用我们高效的模型来协助数据收集和使用新收集的数据来改进模型之间进行迭代。我们接下来介绍每个相互联系的组件，然后是我们创建的数据集以及演示我们方法有效性的实验。

**Task任务**：在NLP和最近的计算机视觉中，基础模型是一种有前途的发展，可以通过使用“prompting提示”技术来执行对新数据集和任务的零样本和少量样本学习。受到这一工作的启发，我们提出了prompted segmentation task可提示的分割任务，其中目标是在给定任何分割prompt提示的情况下返回有效的分割掩码（见图1a）。一个prompt提示只是指定图像中要分割的内容，例如，提示可以包括识别对象的空间或文本信息。有效输出掩码的要求意味着即使提示存在歧义并且可能指向多个对象（例如，衬衫上的一个点可能表示衬衫或穿着它的人），对于这些对象中至少一个，输出也应该是合理掩码。我们将可提示的分割任务用作预训练目标，并通过提示工程来解决一般的下游分割任务。

**Model模型**：可提示的分割任务和现实世界使用的目标对模型架构施加了约束。特别是，模型必须支持灵活的提示，需要在摊销的实时计算掩码以允许交互式使用，并且必须是模糊感知的。令人惊讶的是，我们发现一个简单的设计满足了这三个约束：一个强大的图像编码器计算image embedding图像嵌入，一个提示编码器嵌入提示，然后两个信息源在轻量级的掩码解码器中组合，预测分割掩码。我们将这个模型称为Segment Anything Model，或SAM（见图1b）。通过将SAM分为图像编码器和快速提示编码器/掩码解码器，可以重用相同的image embedding图像嵌入（并摊销其成本）与不同的提示。给定image embedding图像嵌入，提示编码器和掩码解码器可以在web浏览器中预测掩码。我们专注于点、框和掩码提示，并且还提供了使用自由形式文本提示的初始结果。为了使SAM模糊感知，我们设计它来预测单个提示的多个掩码，从而使SAM能够自然地处理模糊性，例如衬衫与人的例子。

**Data engine数据引擎**：为了达到对新数据分布的强大泛化，我们发现需要在大量和多样化的掩码上训练SAM，超出任何现有的分割数据集。虽然获取数据的典型方法是在线获取[82]，但掩码并不自然丰富，因此我们需要另一种策略。我们的解决方案是建立一个“数据引擎”，即我们与model-in-the-loop数据集标注共同开发我们的模型（见图1c）。我们的数据引擎有三个阶段：assisted-manual辅助手动，semi-automatic半自动和fully automatic完全自动。在第一阶段，SAM协助标注人员标注掩码，类似于经典的交互式分割设置。在第二阶段，SAM可以通过prompting提示可能的对象位置来自动为一部分对象生成掩码，并且标注员专注于标注其余对象，从而帮助增加掩码多样性。在最后一个阶段，我们用前景点的常规网格去提示SAM，平均每个图像产生约100个高质量掩码。

**Dataset数据集**：我们最终的数据集SA-1B包含来自1100万张许可和隐私保护图像的超过10亿个掩码（见图2）。SA-1B是完全自动使用我们数据引擎的最后一个阶段收集的，比任何现有的分割数据集都有400倍多的掩码，并且我们广泛验证了掩码的高质量和多样性。除了将SA-1B用于训练SAM以变得强大和普遍之外，我们希望SA-1B成为研究新的基础模型的有价值的资源。

<div align=center>
    <img src="zh-cn/img/ch3/p2.png" /> 
</div><p align=center>图2. 来自我们新引入的数据集SA-1B的覆盖掩码的示例图像。SA-1B包含1100万多样化、高分辨率、许可和隐私保护图像和11亿个高质量的分割掩码。</p>

**Responsible 负责的AI**：我们研究并报告了使用SA-1B和SAM时可能存在的公平性问题和偏见。 SA-1B中的图像跨越了地理和经济上多样化的国家，我们发现SAM在不同人群中的表现相似。 我们希望这将使我们的工作对于现实世界的使用案例更加公平。 我们在附录中提供了模型和数据集卡片。

**Experiments实验**：我们广泛评估SAM。首先，使用一套新的23个多样化的分割数据集，我们发现SAM从单个前景点产生高质量的掩码，通常只是略低于手动标注的Ground Truth真值。其次，我们在零样本迁移协议下使用提示工程发现了各种下游任务一致强大的定量和定性结果，包括edge detection边缘检测，object proposal generation对象建议生成，instance segmentation实例分割和text-to-mask prediction文本到掩码预测的初步探索。这些结果表明，SAM可以在提示工程下使用，以解决涉及对象和图像分布的各种任务，超出SAM的训练数据。然而，仍有改进的空间，我们在第8节中讨论。

**Release发布**：我们为研究目的发布SA-1B数据集，并在Apache 2.0许可下发布SAM，地址为https://segment-anything.com 。我们还使用在线演示展示了SAM的功能。


#### 2.分割一切任务(Segment Anything Task)

我们从NLP自然语言处理中获得灵感，其中下一个token预测任务用于基础模型预训练，并通过提示工程来解决各种下游任务。为了构建分割的基础模型，我们的目标是定义具有类似功能的任务。

**Task任务** ：我们首先将prompt提示的思想将NLP迁移到分割，其中一个prompt提示可以是一组前景/背景点，粗略的框或掩码，自由形式的文本，或者，一般来说，任何指示在图像中分割什么的信息。然后，promptable segmentation task可提示分割任务是给定任何提示返回有效的分割掩码。掩码的“有效”要求只是意味着即使提示模糊并且可能指向多个对象（例如，回想一下衬衫与人的例子，并参见图3），输出也应该是至少一个这些对象中的合理掩码。这个要求类似于期望语言模型输出对模糊提示的连贯响应。我们选择这个任务是因为它导致了一种自然的预训练算法和一种通过提示将可提示分割任务迁移到下游分割任务的通用方法。

<div align=center>
    <img src="zh-cn/img/ch3/p3.png" /> 
</div><p align=center>图3. 每一列显示由SAM从一个模糊点提示（绿色圆圈）生成的3个有效掩码。</p>

**Pre-training预训练** ：可提示的分割任务提出了一种自然的预训练算法，该算法为每个训练样本模拟一系列提示（例如，点，框，掩码），并将模型的掩码预测与真值GroundTruth进行比较。我们从交互式分割中采用了这种方法，尽管与交互式分割不同，其目标是在足够的用户输入后最终预测有效的掩码，**我们的目标是始终为任何提示预测有效的掩码，即使提示是模糊的**。这确保了预训练的模型在涉及模糊性的用例中是有效的，包括我们的data engine数据引擎所需的自动注释。我们指出，良好地执行此任务是具有挑战性的，并且需要专门的建模和训练损失选择，我们在附录3中讨论。

**Zero-shot transfer零样本迁移** ：直观地说，我们的预训练任务赋予模型在推理时间对任何提示做出适当响应的能力，因此下游任务可以通过设计适当的提示来解决。例如，如果有一个猫的bounding box检测器，那么可以通过将检测器的框输出作为提示提供给我们的模型来解决猫的实例分割。一般来说，各种实际分割任务都可以作为提示来处理。除了自动数据集标记之外，我们在附录7中的实验部分探索了五个不同的示例任务。

**Related tasks(相关任务)** : 分割是一个广泛的领域：有interactive segmentation(交互式分割)，edge detection(边缘检测)，super pixelization(超像素化)，object proposal generation(对象建议生成)，foreground segmentation(前景分割)，semantic segmentation(语义分割)，instance segmentation(实例分割)，panoptic segmentation(全景分割)等。我们的**可提示分割任务**的目标是产生一个广泛的模型，可以通过提示工程来适应许多（但不是所有）现有和新的分割任务。这种能力是一种任务泛化的形式。请注意，这与以前的多任务分割系统的工作不同。在多任务系统中，单个模型执行固定的一组任务，例如，联合语义，实例和全景分割，但训练和测试任务是相同的。我们工作中的一个重要区别是，训练可提示分割的模型可以通过作为更大系统中的组件来执行新的，不同的任务，例如，要执行实例分割，可提示分割模型与现有的对象检测器结合。

**Discussion(讨论)** :prompting提示和composition组合是强大的工具，可以使单个模型以可扩展的方式使用，可能用于在模型设计时不知道的任务。这种方法类似于其他基础模型的使用方式，例如，CLIP是DALL·E图像生成系统的文本图像对齐组件。**我们预计，由提示工程等技术驱动的可组合系统设计将使系统能够实现比专门为固定任务集训练的系统更广泛的应用**。通过比较可提示和交互式分割的提示和组合来比较它们是有趣的：虽然交互式分割模型是为人类用户而设计的，但训练可提示分割的模型也可以组合成更大的算法系统，如我们将展示的那样。

#### 3.分割一切模型(Segment Anything Model)

我们接下来描述了用于可提示分割的Segment Anything Model (SAM)模型的三个组件，如图4所示：图像编码器，灵活的提示编码器和快速的掩码解码器。我们建立在Transformer视觉模型之上，这些模型具有特定的权衡，以实现（摊销）实时性能。我们在这里对这些组件进行高层次的描述，详细信息见附录A。

<div align=center>
    <img src="zh-cn/img/ch3/p4.png" /> 
</div><p align=center>图4. 分割一切模型（SAM）概述。重量级图像编码器输出图像嵌入，然后可以通过各种输入提示有效地查询，以平摊的实时速度产生对象掩码。对于对应于多个对象的模糊提示，SAM可以输出多个有效的掩码和相关的置信度分数。</p>

**Image encoder(图像编码器)** :受到可扩展性和强大的预训练方法的启发，我们使用了一个MAE预训练的Vision Transformer（ViT），最小化地适应处理高分辨率输入。图像编码器每个图像运行一次，并且可以在提示模型之前应用。

**Prompt encoder(提示编码器)** :我们考虑两组提示：稀疏（点，框，文本）和密集（掩码）。我们通过位置编码来表示点和框，这些编码与每种提示类型的学习嵌入相加，并且使用CLIP的现成文本编码器来表示自由文本。密集提示（即掩码）使用卷积嵌入，并与图像嵌入元素相加。

**Mask decoder(掩码解码器)** :掩码解码器通过有效地将image embedding图像嵌入，prompt embeddings提示嵌入和输出token映射到掩码来实现。这种设计采用了一个修改的Transformer解码器块，后跟一个动态掩码预测头。我们修改的解码器块使用**提示自注意力和交叉注意力两个方向**（提示到图像嵌入和反之亦然）来更新所有嵌入。在运行两个块之后，我们对图像嵌入进行上采样，并且MLP将输出token映射到动态线性分类器，然后在每个图像位置计算掩码前景概率。

**Resolving ambiguity(解决歧义)** :在一个输出中，如果给出了歧义的提示，模型将平均多个有效的掩码。为了解决这个问题，我们修改了模型，以便为单个提示预测多个输出掩码（见图3）。我们发现，3个掩码输出足以解决大多数常见情况（嵌套的掩码通常最多三层：whole整体，part部分和subpart子部分）。在训练期间，我们仅在掩码上反向传播最小损失。为了对掩码进行排序，模型预测每个掩码的置信度分数（即估计的IoU）。

**Efficiency(效率)** :整体模型设计主要受到效率的启发。给定预先计算的图像嵌入，提示编码器和掩码解码器在Web浏览器中以CPU运行，运行时间大约为50毫秒。这种运行时性能使我们的模型能够无缝、实时地提示我们的模型。

**Losses and training(损失和训练)** :我们使用focal loss和dice loss的线性组合来监督掩码预测。使用geometric prompts几何提示的混合来训练可提示的分割任务（对于文本提示，请参见第7.5小节）。我们通过在每个掩码中随机采样11轮提示来模拟交互式设置，从而使SAM能够无缝地集成到我们的数据引擎中。

<div align=center>
    <img src="zh-cn/img/ch3/p5.png" /> 
</div>

#### 4.分割一切数据引擎(Segment Anything Data Engine)

由于分割掩码在互联网上并不丰富，我们建立了一个数据引擎来收集我们的1100万掩码数据集SA-1B。数据引擎有三个阶段：
1. 模型辅助的manual annotation stage手动标注阶段
2. 自动预测掩码和模型辅助标注的semi-automatic stage半自动阶段
3. 在没有标注人员输入的情况下，我们的模型生成掩码的fully automatic stage完全自动阶段。

我们接下来详细介绍每个阶段。

**Assisted-manual stage（人工辅助阶段）**：在第一阶段，类似于经典的交互式分割，一组专业的标注人员使用SAM提供的基于浏览器的交互式分割工具，通过点击前景/背景对象点来标记掩码。掩码可以使用像素精确的“brush画笔”和“eraser橡皮擦”工具进行精炼。我们的模型辅助标注在浏览器中实时运行（使用预先计算的图像嵌入），从而实现真正的交互式体验。我们没有对标注对象施加语义约束，标注员自由地标注“stuff物体”和“things事物”。我们建议标注员标注他们能够命名或描述的对象，但是我们没有收集这些名称或描述。标注员被要求按重要性顺序标注对象，并且鼓励在一个掩码花费超过30秒来标注之后，继续进行下一个图像。

在这个阶段开始时，SAM使用常见的公共分割数据集进行训练。在收集足够的数据标注之后，SAM仅使用新标注的掩码进行重新训练。随着收集的掩码越来越多，图像编码器从ViT-B扩展到ViT-H，其他架构细节也不断发展；总共我们重新训练了6次我们的模型。每个掩码的平均标注时间从34秒降低到14秒，因为模型得到了改进。我们注意到，14秒比COCO的掩码标注快6.5倍，而且只比使用极端点进行边界框标注慢2倍。随着SAM的改进，每个图像的平均掩码数量从20增加到44个掩码。总的来说，我们在这个阶段收集了12万张图像中的430万个掩码。

**Semi-automatic stage（半自动阶段）**：在这个阶段，我们的目标是增加掩码的多样性，以提高我们的模型的任意分割能力。为了让标注员专注于不那么突出的对象，我们首先自动检测置信度高的掩码。然后我们向标注员展示了图像，这些图像已经预先填充了这些掩码，并要求他们标注任何未标注的对象。为了检测置信度高的掩码，我们在所有第一阶段的掩码上使用通用的“object对象”类别训练了一个bounding box边界框检测器。在这个阶段，我们收集了额外的590万个掩码，18万张图像（总共1020万个掩码）。与第一阶段一样，我们定期在新收集的数据上重新训练我们的模型（5次）。每个掩码的平均标注时间又回到了34秒（不包括自动掩码），因为这些对象更难标注。每个图像的平均掩码数量从44个掩码增加到72个掩码（包括自动掩码）。

**Fully automatic stage(完全自动阶段)**:在最终阶段，标注是完全自动的。这是由于我们模型的两个主要改进而实现的。首先，在这个阶段开始时，我们已经收集了足够的掩码来大大改进模型，包括前面阶段的多样化掩码。其次，在这个阶段，我们已经开发了ambiguity-aware模糊感知模型，这使我们能够在模糊的情况下预测有效的掩码。具体来说，我们使用32x32的常规网格点提示模型，并为每个点预测可能对应于有效对象的一组掩码。使用模糊感知模型，如果一个点位于一个part部分或subpart子部分上，我们的模型将返回subpart子部分，part部分和whole整个对象。我们模型的IoU预测模块用于选择confident置信的掩码；此外，我们还识别并选择stable稳定的掩码（如果阈值化概率图在$0.5-\delta$和$0.5+\delta$之间的结果与相似的掩码，则认为掩码是稳定的）。最后，在选择置信和稳定的掩码之后，我们应用非极大值抑制（non-maximal suppression，NMS）来过滤重复项。为了进一步提高较小掩码的质量，我们还处理了多个重叠的放大图像裁剪。有关此阶段的更多详细信息，请参见附录B。我们将完全自动的掩码生成应用于我们数据集中的所有1100万图像，产生了11亿个高质量掩码的总量。我们接下来描述和分析了这个结果数据集SA-1B。

#### 5.分割一切数据集(Segment Anything Dataset)

我们的数据集SA-1B包含1100万多样化的、高分辨率的、许可的和隐私保护的图像和11亿个高质量的分割掩码，这些掩码是通过我们的数据引擎收集的。我们将SA-1B与现有数据集进行比较，并分析掩码的质量和属性。我们将SA-1B发布出来，以帮助未来计算机视觉的基础模型的发展。我们注意到，SA-1B将在某些研究用途下以有利的许可协议发布，并为研究人员提供保护。

**Images图像**:我们从直接与摄影师合作的提供商那里获得了一组新的1100万张图像。这些图像具有高分辨率（平均3300x4950像素），因此，数据量的大小可能会带来可访问性和存储挑战。因此，我们将发布下采样的图像，其最短边设置为1500像素。即使是下采样后，我们的图像也比许多现有的视觉数据集（例如COCO图像是480x640像素）具有更高的分辨率。请注意，今天的大多数模型都在更低的分辨率输入上运行。人脸和车牌已被模糊处理。

**Masks掩码**:我们的数据引擎产生了11亿个掩码，其中99.1%是完全自动的。因此，自动掩码的质量是至关重要的。我们直接将它们与专业标注进行比较，并查看各种掩码属性与突出的分割数据集的比较。我们的主要结论，如下面的分析和第7节中的实验所证实的那样，是我们的自动掩码是高质量的，并且对于训练模型是有效的。受这些发现的启发，SA-1B仅包括自动生成的掩码。

**Mask quality掩码质量**:我们随机抽取了500张图像（约5万个掩码），并要求我们的专业标注者改进这些图像中所有掩码的质量。标注者使用我们的模型和像素精确的“brush画笔”和“eraser橡皮擦”编辑工具来完成此操作。这个过程产生了一对自动预测的和专业修正的掩码。我们计算了每对之间的IoU，并发现94%的对有大于90%的IoU（97%的对有大于75%的IoU）。相比之下，以前的工作估计了标注者之间的一致性在85-91%的IoU。我们在第7节中的实验通过人类打分证实了掩码质量相对于各种数据集是高的，并且在自动掩码上训练我们的模型几乎与使用数据引擎生成的所有掩码一样好。

<div align=center>
    <img src="zh-cn/img/ch3/p6.png" /> 
</div><p align=center>图5. 图像大小正则化的掩码中心分布</p>

**Mask properties(掩码属性)**:在图5中，我们将SA-1B中的对象中心的空间分布与最大的现有分割数据集进行了比较。所有数据集中都存在常见的摄影师偏见。我们观察到，与LVIS v1和ADE20K相比，SA-1B在图像角落的覆盖率更大，而COCO和Open Images V5则具有更突出的中心偏差。在图6（图例）中，我们将这些数据集进行比较。 SA-1B比第二大的Open Images数据集有11倍多的图像和400倍多的掩码。平均而言，它比Open Images有36倍多的掩码每张图像。在这方面最接近的数据集是ADE20K，但仍然比Open Images有3.5倍少的掩码每张图像。图6（左）绘制了每张图像的掩码分布。接下来，我们在图6（中）中查看图像相对掩码大小（掩码面积除以图像面积的平方根）。正如预期的那样，由于我们的数据集每张图像都有更多的掩码，因此它也倾向于包含更大百分比的小型和中型相对大小掩码。最后，为了分析形状复杂性，我们在图6（右）中查看掩码凹陷（1减去掩码面积，除以掩码凸包面积）。由于形状复杂性与掩码大小相关，因此我们通过对掩码大小分布进行分层采样来控制数据集的掩码大小分布。我们观察到，我们的掩码的凹陷分布与其他数据集的凹陷分布大致相似。

<div align=center>
    <img src="zh-cn/img/ch3/p7.png" /> 
</div><p align=center>图6. 数据集掩码属性。该图引用了每个数据集中的图像和掩码数量。注意，与现有最大的分割数据集Open Images相比，SA-1B拥有多11倍的图像以及多400倍的掩码。</p>

#### 6.分割一切负责任AI分析(Segment Anything RAI Analysis)

我们接下来通过调查使用SA-1B和SAM时可能存在的公平性问题和偏见来对我们的工作进行负责任的人工智能（RAI）分析。我们专注于SA-1B的地理和收入分布以及SAM在人们的受保护属性上的公平性。我们还在第F节中提供了数据集、数据标注和模型卡。

<div align=center>
    <img src="zh-cn/img/ch3/p8.png" /> 
</div><p align=center>图7. SA-1B图像估计的地理分布。世界上大多数国家在SA-1B中都有超过1000张图像，而且三个拥有最多图像的国家来自世界的不同地区。</p>

**Geographic and income representation(地理和收入表示)**:我们使用标准方法推断图像拍摄的国家（见附录C）。在图7中，我们可视化了SA-1B中每个国家的图像计数（左）和最多图像的50个国家（右）。我们注意到，前三个国家来自世界的不同地区。接下来，在表1中，我们比较了SA-1B、COCO和Open Images的地理和收入表示。 SA-1B的图像中有大量的欧洲和亚洲和大洋洲的图像，以及中等收入国家的图像。所有数据集都低估了非洲以及低收入国家。我们注意到，在SA-1B中，所有地区，包括非洲，都至少有2800万个掩码，比任何以前的数据集的总掩码数多10倍。最后，我们观察到，每张图像的平均掩码数（未显示）在地区和收入方面相当一致（每张图像94-108个掩码）。

<div align=center>
    <img src="zh-cn/img/ch3/p9.png" /> 
</div><p align=center>表1. 对地理和收入表示的比较。 SA-1B在欧洲，亚洲&大洋洲以及中等收入国家的表示更高。 非洲、拉丁美洲和加勒比以及低收入国家的图像在所有数据集中都被低估。</p>

**Fairness in segmenting people(分割人员中的公平性)**:我们通过测量SAM在群体之间的性能差异来调查可能存在的公平性问题，这些群体包括perceived gender presentation(感知性别表现)、 perceived age group(感知年龄组)和perceived skin tone(感知肤色)。我们使用More Inclusive Annotations for People（MIAP）数据集来测量性别表现和年龄以及用于肤色的专有数据集（见附录C）。我们的评估使用随机采样1和3个点的模拟交互分割（见附录D）。表2（左上）显示了感知性别表现的结果。我们注意到，女性在检测和分割数据集中被证明是被低估的，但观察到SAM在群体之间表现相似。我们在表2（左下）中重复分析感知年龄，注意到那些被认为年轻和年长的人在大规模数据集中被证明是被低估的。 SAM在被认为年长的人中表现最好（尽管置信区间很大）。最后，我们在表2（右侧）中重复分析感知肤色，注意到那些肤色较浅的人被证明在大规模数据集中被过度代表，而那些肤色较深的人则被低估。由于MIAP不包含感知肤色标注，因此我们使用一个专有数据集，该数据集包含感知Fitzpatrick肤色类型的标注，该类型范围从1（最浅的肤色）到6（最深的肤色）。虽然平均值有所不同，但我们没有发现群体之间的显着差异。我们相信我们的发现源于任务的性质，我们承认当SAM用作更大系统的组件时，可能会出现偏见。最后，在附录C中，我们将分析扩展到分割服装，其中我们发现了感知性别表现的偏见。

<div align=center>
    <img src="zh-cn/img/ch3/p10.png" /> 
</div><p align=center>表2. SAM的性能，分割人员，包括感知性别表现、年龄组和肤色。显示了95％的置信区间。在每个分组中，除了年长的人与中年人之外，所有置信区间都重叠。</p>

#### 7.零样本迁移实验(Zero-Shot Transfer Experiments)

我们在本节中使用SAM（Segment Anything Model）进行零样本迁移实验。我们考虑了五个任务，其中四个与用于训练SAM的可提示的分割任务有很大不同。这些实验评估了SAM在训练期间未见过的数据集和任务上（我们对“零样本迁移”的使用遵循了CLIP中的使用）。数据集可能包括新的图像分布，例如水下或自我中心的图像（例如图8），我们所知道的是，这些图像在SA-1B中不存在。

<div align=center>
    <img src="zh-cn/img/ch3/p11.png" /> 
</div><p align=center>图8. 来自23个不同的分割数据集的样本，用于评估SAM的零样本迁移能力</p>

我们的实验从测试可提示分割的核心目标开始：从任何提示中产生有效的掩码。我们强调单个前景点提示的具有挑战性的情况，因为它比其他更具体的提示更可能是模糊的。接下来，我们将展示一系列实验，这些实验遍历了低、中和高级图像理解，并大致平行于该领域的历史发展。具体来说，我们提示SAM
1. 执行边缘检测
2. 分割所有内容，即object proposal generation对象建议生成
3. 分割检测到的对象，即 instance segmentation实例分割
4. 作为一种proof-of-concept概念验证，从自由形式文本中分割对象。这四个任务与SAM在训练时的可提示分割任务有很大不同，并通过提示工程实现

我们的实验最后进行了一个消融研究。

**Implementation(实现)**:除非另有规定，否则
1. SAM使用MAE预训练的ViT-H图像编码器
2. 在SA-1B上进行训练，注意，该数据集仅包含来自我们数据引擎最后阶段的自动生成的掩码
3. 对于所有其他模型和训练细节（例如超参数），请参阅附录A。

##### 7.1 零样本单点有效掩码评估(Zero-Shot Single Point Valid Mask Evaluation)

**Task(任务)**:我们评估从单个前景点分割对象。这个任务是不确定的，因为一个点可以指向多个对象。大多数数据集中的真值掩码不会枚举所有可能的掩码，这可能会使自动指标不可靠。因此，我们补充了标准的mIoU指标（即预测和真值掩码之间所有IoUs的平均值），以及一个人类研究，其中标注者从1（无意义）到10（像素完美）评估掩码质量。有关其他详细信息，请参见附录D.1，附录E和附录G。

默认情况下，我们从真值掩码的“中心”（在掩码的内部距离变换的最大值处）采样点，遵循交互式分割中的标准评估协议。由于SAM能够预测多个掩码，因此我们默认情况下仅评估模型的最有信心的掩码。基线都是单掩码方法。我们主要与RITM进行比较，这是一种强大的交互式分割器，与我们的基准相比，它在其他强大的基线中表现最佳。

**Datasets(数据集)**:我们使用了一个新编译的23个数据集的套件，具有多样化的图像分布。图8列出了数据集并显示了每个数据集的样本（有关更多详细信息，请参见附录表7）。我们使用所有23个数据集进行mIoU评估。对于人类研究，我们使用图9b中列出的子集（由于这种研究的资源要求）。这个子集包括SAM根据自动指标优于和低于RITM的数据集。

**Results(结果)**:首先，我们查看了整套23个数据集使用mIoU的自动评估结果。我们将图9a中的每个数据集的结果与RITM进行比较。SAM在23个数据集中的16个数据集上产生了更高的结果，最多约为47IoU。我们还提供了一个“oracle”结果，其中SAM的3个掩码中最相关的一个是通过将它们与真值GT进行比较来选择的，而不是选择最有信心的掩码。这揭示了模糊性对自动评估的影响。特别是，通过oracle来执行模糊性解决方案，SAM在所有数据集上均优于RITM。

人类研究中的结果显示在图9b中。误差条是95％置信区间的平均掩码得分（所有差异都是显着的；有关详细信息，请参见附录E）。我们观察到标注者始终将SAM的掩码的质量与最强的基线RITM相比。SAM的单输出掩码具有明显较低的评级，尽管仍然高于RITM。 SAM的平均评级在7到9之间，这对应于定性评级指南：“高分（7-9）：对象是可识别的，错误很少（例如，丢失一个小的，被遮挡的断开的组件，...）”。这些结果表明SAM已经学会从单个点分割有效的掩码。请注意，对于像DRAM和IBD这样的数据集，SAM在自动指标上表现较差，但在人类研究中仍然获得了高评级。

图9c显示了其他基线，SimpleClick和FocalClick，这些基线比RITM和SAM的单点性能低。随着点数从1增加到9，我们观察到方法之间的差距减小。这是预期的，因为任务变得更容易；此外，SAM没有针对非常高的IoU范围进行优化。最后，在图9d中，我们用随机点采样替换默认的中心点采样。我们观察到SAM和基线之间的差距增大，并且SAM能够在任何采样方法下获得可比的结果。

<div align=center>
    <img src="zh-cn/img/ch3/p12.png" /> 
</div><p align=center>图9. 23个数据集上点到掩码评估。a.SAM和最强的单点分割器RITM的平均IoU。由于模糊性单个掩码可能无法匹配真值；圆圈显示SAM3个预测中最相关的oracle结果。b. 每个数据集掩码质量评级由标注者从1到10进行比较。所有方法都使用真值掩码中心作为提示。c，d的mIoU随着点数的增加而变化。</p>

##### 7.2 零样本边缘检测(Zero-Shot Edge Detection)

**Approach(方法)**:我们使用BSDS500评估SAM在经典的低级任务中的边缘检测。我们使用我们的自动掩码生成pipeline管线的简化版本。具体来说，我们用16x16的常规网格的前景点提示SAM，从而产生768个预测的掩码（每个点3个）。通过NMS删除冗余的掩码。然后，使用未阈值的掩码概率图的Sobel滤波和标准轻量级后处理计算边缘图，包括边缘NMS（有关详细信息，请参见附录D.2）。

<div align=center>
    <img src="zh-cn/img/ch3/p13.png" /> 
</div><p align=center>图10. 在BSDS500上的零样本边缘预测。SAM没有被训练来预测边缘地图，在训练期间也没有访问BSDS图像或标注。</p>

**Results(结果)**:图10显示了SAM在BSDS500上的边缘检测结果。我们观察到SAM产生了合理的边缘图，尽管它没有为边缘检测进行训练。与真值相比，SAM预测了更多的边缘，包括BSDS500中未标注的合理边缘。这种偏差在表3中的量化结果中得到反映：在50%precision准确率（R50）处的recall召回率很高，以precision精度为代价。SAM自然落后于学习BSDS500的偏差的最先进的方法，即要抑制的边缘。尽管如此，SAM的性能与HED（也在BSDS500上进行了训练）和先前的零样本迁移方法相比仍然很好。

<div align=center>
    <img src="zh-cn/img/ch3/p14.png" /> 
</div><p align=center>表3. 在BSDS500上的零样本迁移到边缘检测</p>

##### 7.3. 零样本对象建议(Zero-Shot Object Proposals)

**Approach(方法)**:接下来，我们评估SAM在中级任务上的对象建议生成。这个任务在对象检测研究中起着重要的作用，作为先驱系统的中间步骤。为了生成object proposal对象建议，我们运行了我们的自动掩码生成管线稍微修改后的版本，并将掩码作为提案输出（详见附录D.3）。

我们在LVIS v1上计算标准的平均召回（AR）指标。我们专注于LVIS，因为它的大量类别提供了一个具有挑战性的测试。我们将其与实现为ViTDet检测器的强大基线进行比较（带有级联Mask R-CNN ViT-H）。我们注意到，这个“baseline基线”对应于“Detector Masquerading
as Proposal generator检测器伪装为提案生成器”（DMP）方法，该方法被证明可以欺骗AR，使其成为一个真正具有挑战性的比较。

**Results(结果)**:在表4中，我们看到不出所料地，使用ViTDet-H的检测结果作为对象建议（即DMP方法欺骗AR）在整体上表现最好。然而，SAM在几个指标上表现出了惊人的成绩。值得注意的是，它在中等和大型对象以及罕见和常见对象上优于ViTDet-H。事实上，SAM仅在小对象和频繁对象上低于ViTDet-H，其中ViTDet-H可以轻松地学习LVIS特定的标注偏差，因为它是在LVIS上训练的，而SAM不是。我们还与SAM的无歧义版本（“单一输出”）进行了比较，该版本在所有AR指标上的表现都比SAM差得多。

<div align=center>
    <img src="zh-cn/img/ch3/p15.png" /> 
</div><p align=center>表4. 在LVIS v1上生成对象建议。SAM应用于零样本，即它没有进行对象建议生成的训练，也没有访问LVIS图像或标注。</p>

##### 7.4. 零样本实例分割(Zero-Shot Instance Segmentation)

**Approach(方法)**:我们将SAM作为实例分割器的分割模块。实现很简单：我们运行一个对象检测器（之前使用的ViTDet），并用它的输出框提示SAM。这说明了在更大的系统中组合SAM。

**Results(结果)**:我们比较了SAM和ViTDet在COCO和LVIS上预测的掩码，结果见表5。从mask AP指标来看，我们在两个数据集上都观察到了差距，其中SAM相当接近，尽管肯定落后于ViTDet。通过可视化输出，我们发现SAM掩码通常比ViTDet掩码更具有质量，边界更清晰（见附录D.4和图16）。为了研究这个观察结果，我们进行了额外的人类研究，要求标注者在之前使用的1到10的质量尺度上评估ViTDet掩码和SAM掩码。在图11中，我们观察到SAM在人类研究中始终优于ViTDet。

<div align=center>
    <img src="zh-cn/img/ch3/p16.png" /> 
</div><p align=center>表5. 实例分割结果。使用ViTDet框提示SAM进行零样本分割。完全监督的ViTDet优于SAM，但在更高质量的LVIS掩模上差距缩小。有趣的是，根据人类评级，SAM优于ViTDet（见图11）</p>

<div align=center>
    <img src="zh-cn/img/ch3/p17.png" /> 
</div><p align=center>图11. 我们对ViTDet和SAM的掩码质量评分分布，都适用于LVIS真值。我们还报告了LVIS和COCO的真值质量。图例显示了评分平均值和95%的置信区间。尽管其AP较低（表5），但SAM的评分高于ViTDet，这表明ViTDet利用了COCO和LVIS训练数据中的偏差。</p>

我们假设在COCO上，其中mask AP差距更大，而且真值GT质量相对较低（如人类研究所表明的那样），ViTDet学习了COCO掩码的特定偏差。SAM是零样本方法，无法利用这些（通常是不可取的）偏差。LVIS数据集具有更高质量的真值GT，但仍然存在特定的异质性（例如，掩码不包含孔洞，它们是通过构造的简单多边形）和模态vs.非模态掩码的偏差。同样，SAM没有经过训练，无法学习这些偏差，而ViTDet可以利用它们。

##### 7.5 零样本文本转掩码(Zero-Shot Text-to-Mask)

**Approach(方法)**:最后，我们考虑了一个更高级别的任务：从自由形式的文本中分割对象。这个实验是SAM处理文本提示的能力的概念验证。虽然我们在所有先前的实验中使用了完全相同的SAM，但对于这一个，SAM的训练过程被修改为使其具有文本感知能力，但是以一种不需要新的文本标注的方式。具体来说，对于每个面积大于100x100的手动收集的掩码，我们提取CLIP图像嵌入。然后，在训练期间，我们用提取的CLIP图像嵌入作为其第一个交互来提示SAM。这里的关键观察是，由于CLIP的图像嵌入是训练的，以与其文本嵌入对齐，因此我们可以使用图像嵌入进行训练，但在推理时使用文本嵌入。也就是说，在推理时间，我们通过CLIP的文本编码器运行文本，然后将生成的文本嵌入作为SAM的提示（有关详细信息，请参见附录D.5）。

**Results(结果)**:我们在图12中展示了定性结果。 SAM可以根据简单的文本提示（如“wheel轮子”）以及类似于“beaver tooth grille海狸牙格栅”的短语来分割对象。当SAM仅从文本提示中选择错误的对象时，额外的点通常可以修复预测。

<div align=center>
    <img src="zh-cn/img/ch3/p18.png" /> 
</div><p align=center>图12. 零样本的文本到掩码。SAM可以使用简单而微妙的文本提示。当SAM无法做出正确的预测时，一个额外的点提示可以提供帮助。</p>

##### 7.6 消融实验(Ablations)

我们在我们的23个数据集套件上进行了几个消融实验，使用单个中心点提示协议。请记住，单个点可能是模糊的，而模糊性可能不会在真值GT中表示，因为每个点只包含一个掩码。由于SAM是在零样本迁移设置中运行的，因此SAM的最高排名掩码与数据标注指南结果的掩码之间可能存在系统偏差。因此，我们还报告了与真值GT（“oracle”）相关的最佳掩码。

<div align=center>
    <img src="zh-cn/img/ch3/p19.png" /> 
</div><p align=center>图13. 对我们的数据引擎阶段的消融研究，图像编码器缩放，和训练数据缩放。</p>

在图13（左）中，我们绘制了在数据引擎阶段从累积数据中训练SAM时的性能。我们观察到每个阶段都会增加mIoU。当使用所有三个阶段进行训练时，自动掩码远远多于手动和半自动掩码。为了解决这个问题，我们发现在训练期间对手动和半自动掩码进行10倍的过采样可以获得最佳结果。这种设置使训练变得复杂。因此，我们还测试了第四种设置，只使用自动生成的掩码。在这种数据下，SAM的性能仅略低于使用所有数据（约0.5 mIoU）。因此，我们默认情况下只使用自动生成的掩码来简化训练设置。

在图13（中间），我们查看数据量的影响。完整的SA-1B包含1100万张图像，我们均匀地对其进行采样，以1百万和1万为例进行消融实验。在10万张图像下，我们观察到所有设置下的mIoU都有很大的下降。但是，在1M张图像下，大约是全数据集的10%，我们观察到与使用完整数据集相比的结果。这种数据模式仍然包含大约10亿个掩码，对于许多用例来说可能是一个实用的设置。

最后，图13（右）显示了ViT-B，ViT-L和ViT-H图像编码器的结果。 ViT-H相对于ViT-B有显着改进，但与ViT-L相比只有微弱的增益。此时，进一步的图像编码器扩展似乎没有意义。

#### 8.讨论(Discussion)

**Foundation models(基础模型)**:预训练模型自机器学习早期以来就已经被应用于下游任务。近年来，随着对规模的重视而变得越来越重要，这种模式已经越来越重要，这些模型最近已经被（重新）称为“foundation models(基础模型)”：即“trained on broad data
at scale and are adaptable to a wide range of downstream tasks(在大规模数据上进行训练的模型，并且可以适应广泛的下游任务)”。我们的工作与这个定义很好地相关，但是我们注意到，图像分割的基础模型本质上是一个有限的范围，因为它代表了计算机视觉的一个重要的，但是部分的子集。我们还将我们的方法与文献[8]中的一个方面进行对比，该方法强调了自监督学习在基础模型中的作用。虽然我们的模型是使用自监督技术（MAE）进行初始化的，但是它的大部分功能来自于大规模监督训练。在数据引擎可以扩展可用注释的情况下，例如我们的情况下，监督训练提供了一种有效的解决方案。

**Compositionality(组合性)**:预训练模型可以提供新的功能，甚至超出了训练时的想象。一个突出的例子是CLIP如何作为更大系统的组件，例如DALL·E。我们的目标是使SAM的这种组合变得简单。我们的目标是通过要求SAM预测广泛的分割提示的有效掩码来实现这一点。这种效果是在SAM和其他组件之间创建可靠的接口。例如，MCC可以轻松地使用SAM来分割感兴趣的对象，并实现对3D重建的强大泛化，从单个RGB-D图像中获得。在另一个例子中，SAM可以用可穿戴设备检测到的凝视点来提示，从而实现新的应用。由于SAM能够泛化到新的域，如自我中心图像，这些系统可以在不需要额外训练的情况下工作。

**Limitations(局限性)**:SAM在一般情况下表现良好，但并不完美。它可能会错过细微的结构，在某些时候会幻想出小的不连通的组件，并且不会像更计算密集的方法那样产生清晰的边界，例如“zoom-in放大”。一般来说，当提供了许多点时，我们期望专门的交互式分割方法能够优于SAM。与这些方法不同，SAM旨在实现广泛的使用，而不是高IoU交互式分割。此外，SAM可以实时处理提示，但是当使用重型图像编码器时，SAM的整体性能仍然不是实时的。我们对文本到掩码任务的探索是探索性的，而且并不完全健壮，尽管我们相信可以通过更多的努力来改进它。虽然SAM可以执行许多任务，但不清楚如何设计简单的提示来实现语义和全景分割。最后，有一些特定于领域的工具，我们期望在各自的领域中优于SAM。

**Conclusion(总结)**:Segment Anything项目是一次尝试，将图像分割提升到基础模型的时代。我们的主要贡献是一个新的任务（可提示的分割），模型（SAM）和数据集（SA-1B），这使得这一跃迁成为可能。SAM是否能够达到基础模型的地位，取决于它在社区中的使用方式，但无论如何，我们都希望这项工作的视角、超过10亿个掩码的发布以及我们的可提示分割模型都将有助于开辟前进的道路。

#### 附录A. Segment Anything Model and Task Details

**图像编码器(Image encoder)**: 一般，图像编码器可以成为任何C x N x W大小的图像嵌入。受可扩展性和强大的预训练的启发，我们使用了一个MAE带掩码自编码器[47]预训练的Vision Transformer(ViT)，并对其进行了最小化适应，以处理高分辨率的输入，具体来说，是一个ViT-H/16，具有14x14窗口化的注意力和四个等间隔的全局注意力块。图像编码器的输出是输入图像的16倍缩小的嵌入。由于我们的运行时目标是实时处理每个提示，因此我们可以承担高数量的图像编码器FLOP，因为它们仅针对每个图像而不是每个提示计算一次。

遵循标准做法（例如文献[40]），我们使用1024x1024的输入分辨率，通过重新缩放图像并填充较短的边来获得。因此，图像嵌入是64x64。为了减少通道维度，我们使用1x1卷积来获得256个通道，然后是3×3卷积，也是256个通道。每个卷积后面都跟着一个正则化层。

**提示编码器(Prompt encoder)**: 稀疏提示被映射为如下的256维的向量嵌入。一个点被表示为位置编码与两个学习到的嵌入（表示在前景中还是背景中）之一的总和。一个框被表示为一个embedding pair嵌入对：(1)它的左上角的位置编码与学习到的表示“左上角”的嵌入进行求和，(2)相同的结构，但使用学习到的表示“右下角”的嵌入。最后，为了表示自由形式的文本，我们使用CLIP的文本编码器(一般来说，任何文本编码器都是可能的)。在本节的其余部分，我们将重点讨论几何提示，并在附录D.5中深入讨论文本提示。

稠密提示（即掩码）与图像具有空间对应关系。我们以比输入图像低4倍的分辨率输入掩码，然后使用两个2x2、步幅为2的卷积，输出通道分别为4和16，进一步降低4倍。最后，1x1卷积将通道维度映射到256。每一层都由GELU激活和正则化层分开。然后将掩码和图像嵌入逐元素相加。如果没有掩码提示，则在每个图像嵌入位置上添加一个表示“无掩码”的学习嵌入。

**轻量级掩码解码器(Lightweight mask decoder)**: 该模块有效地将图像嵌入以及一组提示嵌入映射到输出掩码中。为了组合这些输入，我们从Transformer的分割模型中得到启发，并修改一个标准的Transformer解码器。在应用我们的解码器之前，我们首先将学习到的输出token嵌入插入到提示嵌入的集合中，该token嵌入将被用于解码器的输出，类似于[CLS] token。为了简单起见，我们将这些嵌入（不包括图像嵌入）统称为“token”。

我们的解码器设计如图14所示。每个解码器层执行四个步骤：
1. 对token的自注意力
2. 从token（作为查询）到图像嵌入
3. 逐点MLP更新每个token
4. 从图像嵌入（作为查询）到token的交叉注意力。

最后一步使用提示信息更新图像嵌入。在交叉注意力期间，图像嵌入被视为一组642x256维向量。每个自/交叉注意力和MLP都有一个残差连接，正则化层和训练时的0.1的dropout。下一个解码器层从前一层获取更新的token和更新的图像嵌入。我们使用两层解码器。

<div align=center>
    <img src="zh-cn/img/ch3/p20.png" /> 
</div><p align=center>图14. 轻量级掩码解码器的细节。一个两层解码器通过cross-attention来更新图像嵌入和提示标记。然后对图像嵌入进行升级，利用更新后的输出标记来动态预测掩码。（为了图像清晰度没有说明：在每个注意层，位置编码被添加到图像嵌入中，整个原始提示token被重新添加到token查询和键值中）。</p>

为了确保解码器已经访问了关键的几何信息，当位置编码参与注意力层时，它们被添加到图像嵌入中。此外，每当它们参与注意力层时，整个原始提示token（包括它们的位置编码）都被重新添加到更新的token中。这允许对提示token的几何位置和类型都有很强的依赖。

在运行解码器后，我们上采样4倍更新后的图像嵌入，使用两个转置卷积层（现在相对于输入图像缩小了4倍）。然后，token再次关注图像嵌入，我们将更新的输出token嵌入传递给一个小的3层的MLP，该MLP输出一个与放大后图像嵌入的通道维数相匹配的向量。最后，我们预测了一个在升级图像嵌入和MLP输出之间具有空间点乘积的掩码。

该Transformer使用256维的嵌入。Transformer MLP块具有2048的内部维度，但是MLP仅应用于token提示，对于这些token，数量相对较少（很少大于20）。但是，在我们有一个64x64图像嵌入的交叉注意力层中，我们通过减少查询、键和值的通道维度为一半（128维），以提高计算效率。所有的注意力层都使用8个头。

用于放大输出图像嵌入的转置卷积大小是2x2，步幅为2，输出通道维度为64和32，并具有GELU激活。它们由正则化层分开。

**使模型具有模糊感知性(Making the model ambiguity-aware)**: 如前所述，单个输入提示可能是模糊的/歧义的，因为它对应于多个有效的掩码，模型将学习对这些掩码进行平均。我们通过一个简单的修改来消除这个问题：我们不是预测单个掩码，而是使用少量的输出token，并同时预测多个掩码。默认情况下，我们预测三个掩码，因为我们观察到三个层（整体、部分和子部分）通常足以描述嵌套掩码。在训练期间，我们计算真值GT和每个预测掩码之间的损失（稍后描述），但只从最低损失反向传播。这是用于具有多个输出的模型的常见技术。为了在应用程序中进行使用，我们希望对预测的掩码进行排序，因此我们添加了一个小头small head（对额外的输出token进行操作），该头估计了每个预测掩码与其覆盖的对象之间的IoU交并比。

有多个提示的歧义更加罕见，三个输出掩码通常会变得相似。为了在训练时最小化退化损失的计算，并确保单个无歧义的掩码接收到正常的梯度信号，当给出多个提示时，我们只预测单个掩码。这是通过添加第四个输出token来实现的，用于额外的掩码预测。这第四个掩码永远不会返回给单个提示，也是多个提示返回的唯一掩码。

**损失(Losses)**: 我们使用focal loss和dice loss的线性组合来监督掩码预测，比例为20:1。与文献[20,14]中不同，我们观察到在每个解码器层之后的辅助深度监督是无用的。IoU预测头是用IoU预测和预测掩码与真值掩码的IoU之间的均方误差损失来训练的。它被添加到掩码损失中，缩放因子为1.0。

**训练算法(Training algorithm)**: 遵循最近文献[92,37]的方法，我们在训练期间模拟交互式分割设置。首先，以相等的概率随机为目标掩码选择前景点或边界框。点是均匀地从真值掩码中采样的。框被视为真值掩码的边界框，每个坐标都添加了随机噪声，标准偏差等于盒子边长的10%，最大为20个像素。这种噪声配置文件是实例分割等应用程序的合理折衷，这些应用程序在目标对象周围产生一个紧密的框，以及交互式分割，其中用户可能会画一个松散的框。

从第一个提示中做出预测后，后续点从前一个掩码预测和真值掩码之间的误差区域中均匀选择。如果误差区域是假阴性或假阳性，则每个新点都是前景或背景。我们还将前一次迭代的掩码预测作为额外的提示提供给我们的模型。为了为下一次迭代提供最大的信息，我们提供了无阈值掩码logits，而不是二值化的掩码。当返回多个掩码时，传递给下一次迭代并用于采样下一个点的掩码是具有最高预测IoU的掩码。

我们发现在8个迭代采样点之后（我们已经测试了16个）收益递减。此外，为了鼓励模型从提供的掩码中受益，我们还使用了两个没有采样额外点的迭代。其中一个迭代是随机插入在8个迭代采样点之中，另一个迭代总是在最后。这总共有11个迭代：一个采样的初始输入提示，8个迭代采样点，以及两个迭代，其中没有向模型提供新的外部信息，因此它可以学习细化自己的掩码预测。我们注意到，使用相对较大的迭代次数是可能的，因为我们轻量级的掩码解码器仅需要图像编码器计算量的1%，因此每次迭代只会增加很小的开销。这与以前的交互式方法不同，这些方法每次优化器更新只执行一次或几次交互步骤。

**训练配方(Training recipe)**: 我们使用AdamW优化器（ $\beta_1=0.9,\beta_2=0.999$）和线性学习率warmup [42]进行250次迭代，以及阶梯式学习率衰减计划。初始学习率（lr）在warmup之后为8e-4。我们训练90k次迭代（约2 SA-1B epochs），并在60k次迭代时将lr降低10倍，再次在86666次迭代时降低。批量大小为256张图像。为了正则化SAM，我们将权重衰减（wd）设置为0.1，并使用drop path [53]，速率为0.4。我们使用0.8的层次学习率衰减（ld）。没有应用数据增强。我们从一个MAE预训练的ViT-H初始化SAM。由于大型图像编码器和1024×1024输入大小，我们将训练分布在**256个GPU上**。

为了限制GPU内存使用，我们每个GPU训练最多64个随机采样的掩码。此外，我们发现轻度过滤SA-1B掩码以丢弃覆盖图像超过90%的任何掩码在定性上改善了结果。

对于消融和其他训练变体（例如，附录D.5文本到掩码），我们遵循上面的默认配方，如下所示。当仅使用第一个和第二个数据引擎阶段的数据进行训练时，我们使用大规模抖动对输入进行增强，其范围为[0.1,2.0]。直观地说，当训练数据更有限时，数据增强可能是有用的。为了训练ViT-B和ViT-L，我们使用128个GPU分布在128个GPU上的128个GPU进行180k次迭代。我们分别为ViT-B/L设置`lr = 8e−4/4e−4，ld = 0.6/0.8，wd = 0.1，dp = 0.6/0.4`。

#### 附录B. Automatic Mask Generation Details

**裁剪(Cropping)**: 掩码是从完整图像上的32x32点的常规网格和20个额外的缩放图像裁剪中生成的，这些裁剪来自2x2和4x4部分重叠的窗口，分别使用16x16和8x8常规点网格。原始的高分辨率图像用于裁剪（这是我们唯一使用它们的时间）。我们删除了接触裁剪内部边界的掩码。我们在两个阶段应用了标准的基于greedy box的NMS（boxes用于效率）：首先在每个裁剪中，然后在裁剪之间。在裁剪中应用NMS时，我们使用模型的预测IoU对掩码进行排序。在裁剪之间应用NMS时，我们根据掩码的来源裁剪，从最缩放的（即4x4裁剪）到最不缩放的（即原始图像）对掩码进行排序。在这两种情况下，我们使用0.7的NMS阈值。

**过滤(Filtering)**: 我们使用三个过滤器来提高掩码质量。首先，为了只保留有信心的掩码，我们通过模型在88.0的阈值处预测的IoU分数进行了过滤。其次，为了只保留稳定的掩码，我们通过在不同的值处对其进行阈值处理，比较由相同的底层软掩码产生的两个二进制掩码。我们只保留预测（即阈值处理logits为0得到的二进制掩码），只有当其-1和+1阈值掩码对之间的IoU等于或大于95.0时。第三，我们注意到，偶尔会有一个自动掩码覆盖整个图像。这些掩码通常是无趣的，我们通过删除覆盖95%或更多图像的掩码来过滤它们。所有过滤阈值都被选择为既能获得大量掩码，又能获得高质量的掩码，这是通过专业标注人员使用第5节中描述的方法进行判断的。

**后处理(Postprocessing)**: 我们观察到两种错误类型，这两种错误类型很容易通过后处理来减轻。首先，估计有4％的掩码包含小的虚假组件。为了解决这些问题，我们删除了面积小于100像素的连接组件（包括删除整个掩码，如果最大组件低于此阈值）。其次，另外估计有4％的掩码包含小的虚假孔洞。为了解决这些问题，我们用面积小于100像素的孔洞进行填充。孔洞被识别为反转掩码的组件。

**自动掩码生成模型(Automatic mask generation model)**: 我们训练了一个特殊版本的SAM，用于完全自动的掩码生成，它牺牲了一些推理速度，以改善掩码生成的性能。我们注意到我们默认的SAM和用于数据生成的SAM之间的差异：它仅在手动和半自动数据上进行了训练，它的训练时间更长（177656次迭代而不是90k），使用大规模抖动数据增强[40]，模拟交互式训练仅使用点和掩码提示（没有框），并且在训练期间仅对每个掩码采样4个点（将我们的默认值从9减少到4加快了训练迭代，并且对1点性能没有影响，尽管如果使用更多点进行评估，它会损害mIoU），最后掩码解码器使用3层而不是2层。

!> 其他附录B-G可自行阅读原paper!


------

### 2.Segment Anything in Medical Images

关于医学影像中的ASM有非常多的paper其他想干paper的阅读可以联系笔者。本文提出了MedSAM，这是将SAM的成功扩展到医学图像的第一次尝试，目的是创建一个用于分割各种医学目标的通用工具。我们将以该方法测试在消化内镜中的表现。代码已开源！单位：UHN, 多伦多大学, Vector AI

!> github:https://github.com/bowang-lab/MedSAM

!> arxiv: https://arxiv.org/abs/2304.12306

分割一切模型(SAM)已经彻底改变了自然图像分割，但其在医学图像上的性能有限。本文提出了MedSAM，这是将SAM的成功扩展到医学图像的第一次尝试，目的是创建一个用于分割各种医学目标的通用工具。

<div align=center>
    <img src="zh-cn/img/ch3/2-1/p1.png" /> 
</div>

具体来说，首先策划了一个大规模的医学图像数据集，包括11种不同模式的20多万个masks。然后，**开发了一种简单的微调方法，将SAM应用于普通医学图像分割**。如下图所示：

<div align=center>
    <img src="zh-cn/img/ch3/2-1/p2.png" /> 
</div>

实验结果上看到：对21个3D分割任务和9个2D分割任务的综合实验表明，MedSAM在3D和2D分割任务上的Dice Similarity Coefficient（DSC）分别为22.5%和17.6%，优于默认SAM模型。

<div align=center>
    <img src="zh-cn/img/ch3/2-1/p3.png" /> 
</div>

<div align=center>
    <img src="zh-cn/img/ch3/2-1/p4.png" /> 
</div>

下面我们配置MedSAM环境，测试其在消化内镜中的表现。

1. 环境安装

```sh
# 创建一个python3.9的虚拟环境

# 安装torch==2.0及torchvision==0.15.1
git clone https://github.com/bowang-lab/MedSAM

# download pretrain model
https://drive.google.com/drive/folders/1ETWmi4AiniJeWOt6HAsYgTjYv_fkgzoN?usp=drive_link

pip install -e .

pip install PyQt5

```

2. 下载预训练的模型

+ SAM下载地址： https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth
+ MedSAM下载地址：https://drive.google.com/drive/folders/1ETWmi4AiniJeWOt6HAsYgTjYv_fkgzoN?usp=drive_link

如果你要微调SAM可以使用第一个SAM的下载地址，如果你想测试MedSAM的效果你可以使用MedSAM的下载地址。将下载的模型保存在`pretrain`文件夹下。

3. 修改`gui.py`

我们使用PyQt5开发的GUI去测试，我们需要修改`gui.py`下的`MedSAM_CKPT_PATH = "./pretrain/medsam_vit_b.pth"`使其加载我们的模型。

4. 运行PyQt5程序并测试

```sh
python gui.py
```

<div align=center>
    <img src="zh-cn/img/ch3/2-1/p5.png" /> 
</div>